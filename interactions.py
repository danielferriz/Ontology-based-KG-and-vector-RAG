from base_logger import logger
from tools import cleanWords, get_local_name
from tools import handle_logs, clean_node_metadata, remove_special_chars_in_llm_output
from lmstudio import get_embedding
from lmstudio import get_chat_completion
from memgraph_interface import insert_knowledge_graph_nodes_relations, return_graph_labels
from memgraph_interface import return_onProcess_nodes, remove_onProcess_status, combine_similar_group_nodes
from memgraph_interface import create_new_relations, counts_connections_from_a_to_b, return_schema
from rdf_interface import get_subclass_uri, validate_relation, provide_relation_comment
import ast
import json

from postgresql import cosine_vector_search, select_prompt


def clean_output_LLM_list(llm_output:str)->str:
	"""
	Cleans the output generated by an LLM and attmps to make a JSON-formatted response

	Params:
		string (llm_output): Output generated by LLM

	Returns:
		string (recovered_list_text): Cleaned response
	"""
	bracket_counter=0
	temporal_text=""
	recovered_list=[]
	for line in llm_output.replace('{','{\n').replace('}','}\n').split('\n'):
		update_recovered_list_text=False
		if '{' in line:
			bracket_counter+=1
		if '}' in line:
			bracket_counter-=1
			if bracket_counter==0:
				update_recovered_list_text=True
		temporal_text+=line+"\n"
		if update_recovered_list_text:
			#logger.debug(f"temporal_text: {temporal_text}")
			temporal_text=temporal_text[temporal_text.index('{')+1:temporal_text.rindex('}')]
			if temporal_text not in recovered_list:
				recovered_list.append(temporal_text)
			temporal_text=""
	recovered_list_text="["
	for item in recovered_list:
		recovered_list_text+='{'+item+'}, '
	recovered_list_text=recovered_list_text[:-2]+']'
	#logger.debug(f"recovered_list_text: {recovered_list_text}")
	return recovered_list_text



def create_knowledge_graph_with_llm(postgresql_connection, config, graph,  chunk, rdf_graph, rdf_nodes, rdf_edges, local2uri, hierarchy, rel_hierarchy, system_prompt, query):
	"""
	Creates KG in Memgraph using ontology definitions

	Params:
		psycopg2.connection (postgresql_connection): Database connnection
		dict (config): Configuration dictionary using values from .yaml file
		langchain_community.graphs.memgraph_graph.MemgraphGraph (graph): Memgraph knowledge graph
		dict (chunk): Chunk with metadata
		rdflib.Graph (rdf_graph): Ontology graph
		list (rdf_nodes): List of possible nodes
		list (rdf_edges): List of possible edges
		dict (local2uri): Relation between local name and URI
		dict (hierarchy): Dictionary that lists, per ontology class, the set of superclass related to that class
		dict (rel_hierarchy): Dictionary that lists, per ontology relation, the set of superclass related to that relation
		string (system_prompt): Behavior to be adopted by LLM to create KG
		string (query): LLM request to create KG

	Returns:
		Message Code, and Message Text.
	"""
	messages = [
		{"role": "system", "content": system_prompt},
		{"role": "user", "content": query}
	]
	logger.info("Calling LLM for text analysys to create knowledge graph")
	# logger.debug(f"System Prompt: {system_prompt}")
	# logger.debug(f"Query: {query}")
	ai_msg = get_chat_completion(config, messages)    
	jtext=ai_msg
	if config.llm_chat_model.startswith('deepseek'):
		jtext=extract_json_from_deepseek(ai_msg)
	
	logger.info(f"LLM response: {jtext}")
	searching_keys=['text', 'head', 'head_type', 'relation', 'tail', 'tail_type']
	counter_prefix_id=0
	try:
		# Safely parse the input string into a list of dictionaries
		connections = ast.literal_eval(clean_output_LLM_list(jtext))

		if not isinstance(connections, list):
			logger.warning("LLM output is NOT a list, thus, this text will be omitted")
			logger.debug(f"Analyzed text: {jtext}")

		if isinstance(connections, list):
			for conn in connections:
				try:
					if not isinstance(conn, dict):
						raise ValueError("Element is not a dictionary.")
					if 'tail_ype' in conn.keys() and 'tail_type' not in conn.keys():
						conn['tail_type']=conn['tail_ype']
					if 'text' not in conn.keys():
						conn['text']=''
					if not set(searching_keys).issubset(conn.keys()):
						raise KeyError("Model didn't generate required tags."+str(conn.keys()))

					conn['prefix_id']=f"{counter_prefix_id:03d}"
					insert_knowledge_graph_nodes_relations(graph, conn, chunk, rdf_graph, rdf_nodes, rdf_edges, local2uri, hierarchy, rel_hierarchy)
					counter_prefix_id+=1

				except (ValueError, KeyError) as e:
					return handle_logs(errnum=401, errmsg=f"Error: Invalid input format. {e}", logging_level=logger.CRITICAL)
			if rdf_graph is not None:
				attempt_merging(postgresql_connection, config, graph, rdf_graph, local2uri, hierarchy, chunk)
				attmpt_force_new_relations(postgresql_connection, config, graph, rdf_graph, rdf_edges, local2uri, chunk)


	except (SyntaxError, ValueError) as e:
		return handle_logs(errnum=402, errmsg=f"Error: Invalid input format. {e}", logging_level=logger.CRITICAL)

	return handle_logs()

def use_query_buffer_for_llm(postgresql_connection, config, query_buffer, text):
	"""
	Usage of LLM to answer Yes/No questions

	Params:
		psycopg2.connection (postgresql_connection): Database connnection
		dict (config): Configuration dictionary using values from .yaml file
		string (query_buffer): Set of Y/n questions

	Returns:
		LLM response in JSON format
	"""
	system_prompt_for_y_n=select_prompt(postgresql_connection, config, 3, variables={})
	query_buffer=query_buffer[:-2]+"]"
	messages = [
		{"role": "system", "content": system_prompt_for_y_n},
		{"role": "user", "content": query_buffer}
	]
	try:
		logger.debug(f"Asking these questions: {query_buffer}")
		ai_msg = get_chat_completion(config, messages)
		if ai_msg is None:
			raise ValueError("Didn't get response from LM Studio")
		jtext=ai_msg
		if config.llm_chat_model.startswith('deepseek'):
			jtext=extract_json_from_deepseek(ai_msg)
		logger.debug(f"ai_msg similarity: {jtext}")
		return json.loads(jtext)
	except Exception as ex:
		logger.error(f"Fail converting answer '{jtext}' to dictionary")
		return {}

def ask_llm_node_similarity(postgresql_connection, config, questions, text):
	"""
	Usage of LLM to answer a set of Yes/No questions given the current text chunk

	Params:
		psycopg2.connection (postgresql_connection): Database connnection
		dict (config): Configuration dictionary using values from .yaml file
		list (questions): Set of Y/n questions
		string (text): Text chunk

	Returns:
		dict (output): LLM response per each asked question 
	"""

	query_buffer_initial=f"TEXT: \"{text}\"\nQUESTIONS: ["
	query_buffer=query_buffer_initial
	system_prompt_for_y_n=select_prompt(postgresql_connection, config, 3, variables={})
	if len(system_prompt_for_y_n) + len(query_buffer_initial) + 100 >= config.llm_max_tokens:
		logger.error("Prompt for Y/n responses cannot be processes due to limited number of max tokens in configuration file")
		return {}

	llm_json_outputs = []
	for question in questions:
		new_question=f"\"{question['questionId']}: {question['question']}\", "
		tokens_in_question = len(system_prompt_for_y_n) + len(query_buffer) + len(new_question)
		# We add 100 tokens to have a little room since we're leading with approximations
		tokens_in_question = int(tokens_in_question * config.llm_tokens_per_100_characters / 100) + 100
		if tokens_in_question > config.llm_max_tokens:
			if query_buffer_initial == query_buffer:
				logger.error("Empty query buffer")
			else:
				llm_json_outputs.append(use_query_buffer_for_llm(postgresql_connection, config, query_buffer, text))
				query_buffer=query_buffer_initial
		query_buffer+=new_question

	llm_json_outputs.append(use_query_buffer_for_llm(postgresql_connection, config, query_buffer, text))

	output = {}
	for llm_output in llm_json_outputs:
		output|=llm_output

	return output


def find_questions_similar_labels(postgresql_connection, config, kg, rdf_graph, local2uri, chunk):
	"""
	Usage of LLM to detect LLM-generated KG nodes refer to the same subject 

	Params:
		psycopg2.connection (postgresql_connection): Database connnection
		dict (config): Configuration dictionary using values from .yaml file
		langchain_community.graphs.memgraph_graph.MemgraphGraph (kg): Memgraph knowledge graph
		rdflib.Graph (rdf_graph): Ontology graph
		dict (local2uri): Relation between local name and URI
		dict (chunk): Chunk with metadata

	Returns:
		dict (questions): Set of questions to ask the LLM 
	"""
	onProcessNodes=return_onProcess_nodes(kg)
	questions=[]
	counterQuestions=0
	for i in range(len(onProcessNodes)):
		originalType_i=onProcessNodes[i]['originalType']
		originalURI_i = local2uri[originalType_i]
		nodeLeftName=onProcessNodes[i]['nodes']
		for j in range(i+1, len(onProcessNodes)):
			originalType_j=onProcessNodes[j]['originalType']
			originalURI_j = local2uri[originalType_j]
			bottom_type = get_subclass_uri(rdf_graph, originalURI_i, originalURI_j)
			if bottom_type:
				q={}
				q['orignalType'] = originalType_i
				if originalURI_j == bottom_type:
					q['orignalType'] = originalType_j
				counterQuestions+=1
				q['nodeLeft']=onProcessNodes[i]['progressId']
				q['nodeRight']=onProcessNodes[j]['progressId']
				q['questionId']=f"Q{counterQuestions:02d}"
				q['question']=select_prompt(postgresql_connection, config, 4, variables={'nodeLeftName':nodeLeftName, 'nodeRightName':onProcessNodes[j]['nodes']})
				questions.append(q)
	return questions

def ask_llm_and_retrieve_answers(postgresql_connection, config, rdf_graph, local2uri, chunk, questions):
	"""
	Prompts LLM a set of questions, and then, interpret the response of each question

	Params:
		psycopg2.connection (postgresql_connection): Database connnection
		dict (config): Configuration dictionary using values from .yaml file
		rdflib.Graph (rdf_graph): Ontology graph
		dict (local2uri): Relation between local name and URI
		dict (chunk): Chunk with metadata
		dict (questions): Set of questions to ask the LLM 

	Returns:
		dict (similar_groups): Optimal group assignment per node
	"""
	reply=ask_llm_node_similarity(postgresql_connection, config, questions, chunk['text'])
	# We validate for more than 2 synonyms
	similar_groups={}
	node2groups={}
	for rk in reply.keys():
		for q in questions:
			if rk.strip().lower() == q['questionId'].strip().lower():
				if 'y' in reply[rk].lower().strip():
					if q['orignalType'] not in similar_groups.keys():
						similar_groups[ q['orignalType'] ] = []
					if q['nodeLeft'] not in similar_groups[ q['orignalType'] ]:
						similar_groups[ q['orignalType'] ].append(q['nodeLeft'])
					if q['nodeRight'] not in similar_groups[ q['orignalType'] ]:
						similar_groups[ q['orignalType'] ].append(q['nodeRight'])
					if q['nodeLeft'] not in node2groups.keys():
						node2groups[ q['nodeLeft'] ] = []
					if q['orignalType'] not in node2groups[ q['nodeLeft'] ]:
						node2groups[ q['nodeLeft'] ].append(q['orignalType'])
					if q['nodeRight'] not in node2groups.keys():
						node2groups[ q['nodeRight'] ] = []
					if q['orignalType'] not in node2groups[ q['nodeRight'] ]:
						node2groups[ q['nodeRight'] ].append(q['orignalType'])
				break

	for node in node2groups.keys():
		if len(node2groups[node]) > 1:
			logger.info(f"Node '{node}' belongs to multiple classes: {node2groups[node]}. Attempting to identify most representativ class...")
			representative_class=node2groups[node][0]
			i=0
			while representative_class and i<len(node2groups[node]):
				originalType_i= node2groups[node][i]
				originalURI_i = local2uri[originalType_i]
				for j in range(i+1, len(node2groups[node])):
					originalType_j= node2groups[node][j]
					originalURI_j = local2uri[originalType_j]
					bottom_type = get_subclass_uri(rdf_graph, originalURI_i, originalURI_j)
					if not bottom_type:
						representative_class=""
						logger.warning(f"Incompatible classes '[{originalType_i}]', '[{originalType_j}]'. This may lead to future issues.")
						break
					representative_class = originalType_i
					if bottom_type == originalURI_j:
						representative_class = originalType_j
				i+=1
			if representative_class:
				logger.debug(f"Match found: {representative_class}")
				for group in similar_groups.keys():
					if group != representative_class:
						logger.debug(f"Deleting item '{node}' from subgroup '{group}'")
						similar_groups[group].remove(node)
				logger.info("Identification of representative class has been successfully completed.")

	#logger.debug(f"similar_groups: {similar_groups}")
	return similar_groups

def handle_metamerge_insert(subgroups, g2sg, subgroups_id, originalType):
	"""
	Manages the insert statements while merging ontological similar groups through the usage of groups and subgroups

	Params:
		list (subgroups): Ontologically related found classes 
		dict (g2sg): Relation between groups and subgroups
		list (subgroups_id): Subgroups identifier
		string (originalType): Node original type

	Returns:
		dict (similar_groups): Optimal group assignment per node before dict key analysis
	"""
	if originalType not in g2sg.keys():
		new_key = len(subgroups)
		subgroups.append( [originalType] )
		g2sg[originalType]=new_key
		subgroups_id.append(originalType)
		

	return subgroups, g2sg, subgroups_id

def meta_merge(similar_groups, rdf_graph, local2uri):
	"""
	Validates similarity among ontology classes

	Params:
		dict (similar_groups): Optimal group assignment per node before dict key analysis
		rdflib.Graph (rdf_graph): Ontology graph
		dict (local2uri): Relation between local name and URI

	Returns:
		dict (similar_groups_meta): Optimal group assignment per node after dict key analysis
	"""
	#logger.debug(f"similar_groups pre-meta: {similar_groups}")
	similar_groups_meta= {}
	subgroups = []
	g2sg = {}
	subgroups_id = []
	for i in range(len(similar_groups.keys())):
		originalType_i = list(similar_groups.keys())[i]
		originalURI_i = local2uri[originalType_i]
		subgroups, g2sg, subgroups_id = handle_metamerge_insert(subgroups, g2sg, subgroups_id, originalType_i)
		ith_subgroup=g2sg[originalType_i]
		for j in range(i+1, len(similar_groups.keys())):
			originalType_j = list(similar_groups.keys())[j]
			originalURI_j = local2uri[originalType_j]
			bottom_type = get_subclass_uri(rdf_graph, originalURI_i, originalURI_j)
			if bottom_type:
				logger.debug(f"bottom_type: {bottom_type}")
				subgroups[ith_subgroup].append(originalType_j)
				g2sg[originalType_j]=ith_subgroup
				if originalURI_j == bottom_type:
					subgroups_id[ith_subgroup]=originalType_j
			if not bottom_type: 
				subgroups, g2sg, subgroups_id = handle_metamerge_insert(subgroups, g2sg, subgroups_id, originalType_j)
	for i in range(len(subgroups)):
		originalType=subgroups_id[i]
		similar_groups_meta[originalType]=[]
		for similarGroupsOtype in subgroups[i]:
			#similar_groups[ similarGroupsOtype ] --> List of Ids
			for v in similar_groups[ similarGroupsOtype ]:
				if v not in similar_groups_meta[originalType]:
					similar_groups_meta[originalType].append(v)

	#logger.debug(f"similar_groups_meta: {similar_groups_meta}")
	return similar_groups_meta

def attempt_merging(postgresql_connection, config, kg, rdf_graph, local2uri, hierarchy, chunk):
	"""
	Validates if 2 or more nodes are ontologically equivalent, and if so, merge them in one node

	Params:
		psycopg2.connection (postgresql_connection): Database connnection
		dict (config): Configuration dictionary using values from .yaml file
		langchain_community.graphs.memgraph_graph.MemgraphGraph (kg): Memgraph knowledge graph
		rdflib.Graph (rdf_graph): Ontology graph
		dict (local2uri): Relation between local name and URI
		dict (hierarchy): Dictionary that lists, per ontology class, the set of superclass related to that class
		dict (chunk): Chunk with metadata
	"""
	questions = find_questions_similar_labels(postgresql_connection, config, kg, rdf_graph, local2uri, chunk)
	if questions:
		similar_groups = ask_llm_and_retrieve_answers(postgresql_connection, config, rdf_graph, local2uri, chunk, questions)
		if similar_groups:
			similar_groups_redux=meta_merge(similar_groups, rdf_graph, local2uri)
			combine_similar_group_nodes(kg, rdf_graph, local2uri, hierarchy, similar_groups_redux )


def create_question_plausible_relations(postgresql_connection, config, rdf_graph, leftNodeId, leftNodeName, rightNodeId, rightNodeName, relation, local2uri, counter):
	"""
	Creates 1 question to ask LLM to detect if given 2 nodes, there exists any ontology relation currently not represented in KG

	Params:
		psycopg2.connection (postgresql_connection): Database connnection
		dict (config): Configuration dictionary using values from .yaml file
		rdflib.Graph (rdf_graph): Ontology graph
		dict (local2uri): Relation between local name and URI
		dict (hierarchy): Dictionary that lists, per ontology class, the set of superclass related to that class
		dict (chunk): Chunk with metadata

	Returns:
		dict (q): Question to ask to LLM
	"""
	q={}
	q['relationTuple']=(leftNodeId, rightNodeId)
	q['relation']=relation
	comment=provide_relation_comment(rdf_graph, local2uri[relation])
	preamble=''
	_variables={'nodeLeftName':leftNodeName, 'nodeRightName':rightNodeName, 'relation':relation}
	if comment:
		_variables['comment']=comment
	q['questionId']=f"Q{counter:02d}"
	q['question']=select_prompt(postgresql_connection, config, 5, variables=_variables)
	return q

def find_plausible_relations(postgresql_connection, config, kg, rdf_graph, rdf_edges, local2uri, chunk):
	"""
	Creates 1 question to ask LLM to detect if given 2 nodes, there exists any ontology relation currently not represented in KG

	Params:
		psycopg2.connection (postgresql_connection): Database connnection
		dict (config): Configuration dictionary using values from .yaml file
		rdflib.Graph (rdf_graph): Ontology graph
		dict (local2uri): Relation between local name and URI
		dict (hierarchy): Dictionary that lists, per ontology class, the set of superclass related to that class
		dict (chunk): Chunk with metadata

	Returns:
		dict (q): Question to ask to LLM
	"""
	onProcessNodes=return_onProcess_nodes(kg)
	questions=[]
	counterQuestions=0
	for i in range(len(onProcessNodes)):
		originalType_i=onProcessNodes[i]['originalType']
		originalURI_i = local2uri[originalType_i]
		nodeName_i=onProcessNodes[i]['nodes']
		nodeId_i = onProcessNodes[i]['progressId']
		for j in range(i+1, len(onProcessNodes)):
			originalType_j=onProcessNodes[j]['originalType']
			originalURI_j = local2uri[originalType_j]
			nodeName_j=onProcessNodes[j]['nodes']
			nodeId_j=onProcessNodes[j]['progressId']
			connections_i_to_j = []
			connections_j_to_i = []
			for relation in rdf_edges:
				logger.debug(f"Validating relation: {relation} between {nodeName_i} and {nodeName_j}")
				if validate_relation(rdf_graph, originalURI_i, local2uri[relation], originalURI_j):
					numberOfRelations=counts_connections_from_a_to_b(kg, nodeId_i, relation, nodeId_j)
					if numberOfRelations==0:
						connections_i_to_j.append(relation)
				if validate_relation(rdf_graph, originalURI_j, local2uri[relation], originalURI_i):
					numberOfRelations=counts_connections_from_a_to_b(kg, nodeId_j, relation, nodeId_i)
					if numberOfRelations==0:
						connections_j_to_i.append(relation)
			for relation in connections_i_to_j:
				counterQuestions+=1
				q = create_question_plausible_relations(postgresql_connection, config, rdf_graph, nodeId_i, nodeName_i, nodeId_j, nodeName_j, relation, local2uri, counterQuestions)
				questions.append(q)
			for relation in connections_j_to_i:
				counterQuestions+=1
				q = create_question_plausible_relations(postgresql_connection, config, rdf_graph, nodeId_j, nodeName_j, nodeId_i, nodeName_i, relation, local2uri, counterQuestions)
				questions.append(q)
	logger.info(f"relation questions: {questions}")
	return questions


def ask_llm_and_retrieve_answers_for_relations(postgresql_connection, config, chunk, questions):
	"""
	Creates 1 question to ask LLM to detect if given 2 nodes, there exists any ontology relation currently not represented in KG

	Params:
		psycopg2.connection (postgresql_connection): Database connnection
		dict (config): Configuration dictionary using values from .yaml file
		dict (chunk): Chunk with metadata
		dict (questions): Set of questions to ask the LLM 

	Returns:
		dict (same_relations): Set of similar relations found by LLM
	"""
	reply=ask_llm_node_similarity(postgresql_connection, config, questions, chunk['text'])
	# We validate for more than 2 synonyms
	same_relations={}
	for rk in reply.keys():
		for q in questions:
			if rk.strip().lower() == q['questionId'].strip().lower():
				if 'y' in reply[rk].lower().strip():
					if q['relation'] not in same_relations.keys():
						same_relations[ q['relation'] ] = []
					if q['relationTuple'] not in same_relations[ q['relation'] ]:
						same_relations[ q['relation'] ].append( q['relationTuple'] )

				break
	logger.debug(f"same_relations: {same_relations}")
	return same_relations

def attmpt_force_new_relations(postgresql_connection, config, kg, rdf_graph, rdf_edges, local2uri, chunk):
	"""
	Asks LLM for similarity between relations, and if there is, then it will update KG

	Params:
		psycopg2.connection (postgresql_connection): Database connnection
		dict (config): Configuration dictionary using values from .yaml file
		langchain_community.graphs.memgraph_graph.MemgraphGraph (kg): Memgraph knowledge graph
		rdflib.Graph (rdf_graph): Ontology graph
		list (rdf_edges): List of possible edges
		dict (local2uri): Relation between local name and URI
		dict (chunk): Chunk with metadata
	"""
	questions = find_plausible_relations(postgresql_connection, config, kg, rdf_graph, rdf_edges, local2uri, chunk)
	if questions:
		same_relations = ask_llm_and_retrieve_answers_for_relations(postgresql_connection, config, chunk, questions)
		if same_relations:
			create_new_relations(kg, same_relations)
	else:
		logger.debug("Couldn't find any relation that could be made with the given nodes")

def extract_json_from_deepseek(text):
	"""
	Remove tags from text generated by deepseek

	Params:
		string (text): text with tags

	Returns:
		string (text): Simplified text
	"""
	starting_index=0
	if '</think>' not in text:
		logger.info("Response does not include the '<think>' tags")
	else:
		starting_index=ai_msg.content.rindex('</think>') + len('</think>')
	return text[starting_index:].replace('```json','').replace('```python','').replace('```','').lstrip().rstrip()

def chat_loop_vector_questions(config, postgresql_connection):
	"""
	RAG chat using that enables user prompt question and then use VECTOR RAG to answer them

	Params:
		dict (config): Configuration dictionary using values from .yaml file
		psycopg2.connection (postgresql_connection): Database connnection
	"""
	while True:
		user_input = input("You: ")
		if user_input.strip().lower() == "/bye":
			print("Goodbye!")
			break
		if not user_input.strip():
			print("Ask a question to the vector dataset or type '/bye' to exit")
		else:
			response = prettyfi_vector_search_with_llm(config, postgresql_connection, user_input) 
			if response is None:
				logger.error("Couldn't retrieve answer from LLM")
				break
			print("Vector: "+response)



def prettyfi_vector_search_with_llm(config, postgresql_connection, question):
	"""
	Makes a vector similarity search of user question, then uses the vector response as context to power RAG

	Params:
		dict (config): Configuration dictionary using values from .yaml file
		psycopg2.connection (postgresql_connection): Database connnection
		string (question): User asked question

	Returns:
		string (text): Simplified text
	"""
	similar = vector_search(config, postgresql_connection, question)
	context = similar['chunk'].tolist()
	context = ' '.join(context)
	# logger.debug(f"Context:\n{context}")

	behavior=select_prompt(postgresql_connection, config, 6, variables={})

	query = f"Context: {context} Question: {question}"

	messages = [
		{"role": "system", "content": behavior},
		{"role": "user", "content": query}
	]

	ai_msg = get_chat_completion(config, messages)
	return ai_msg

def vector_search(config, postgresql_connection, question):
	"""
	Makes a vector similarity search of user question

	Params:
		dict (config): Configuration dictionary using values from .yaml file
		psycopg2.connection (postgresql_connection): Database connnection
		string (question): User asked question

	Returns:
		dict (similar): Vector database reply
	"""
	text = cleanWords(question)
	question_embedding = get_embedding(config, text)
	similar = cosine_vector_search(postgresql_connection, config, question_embedding)
	return similar


def graph_system_prompt(postgresql_connection, config,  graph, rdf_additional_data):
	"""
	Creates LLM prompt for KG creation

	Params:
		psycopg2.connection (postgresql_connection): Database connnection
		dict (config): Configuration dictionary using values from .yaml file
		langchain_community.graphs.memgraph_graph.MemgraphGraph (graph): Memgraph knowledge graph
		string (rdf_additional_data): Ontology definitions found in file

	Returns:
		string (CYPHER_GENERATION_TEMPLATE): LLM Prompt
	"""
	node_labels, edge_labels=return_graph_labels(graph)
	schema=return_schema(graph)
	if schema is None:
		return None


	CYPHER_GENERATION_TEMPLATE=select_prompt(postgresql_connection, config, 8, variables={'schema': schema, 'node_labels': node_labels, 'edge_labels': edge_labels})

	return CYPHER_GENERATION_TEMPLATE

def graph_search(config, graph, system_prompt, query):
	"""
	Makes a vector similarity search of user question

	Params:
		dict (config): Configuration dictionary using values from .yaml file		
		langchain_community.graphs.memgraph_graph.MemgraphGraph (graph): Memgraph knowledge graph
		string (system_prompt): Behavior to be adopted by LLM to create KG
		string (query): LLM prompt

	Returns:
		dict (similar): Vector database reply
	"""
	logger.debug(f"Creating Cypher with systen prompt: {system_prompt}")
	logger.debug(f"And query: {query}")
	messages = [
		{"role": "system", "content": system_prompt},
		{"role": "user", "content": query}
	]
	logger.info("Calling LLM to create Cypher for Knowledge Base")
	ai_msg = get_chat_completion(config, messages).replace("```cypher","").replace("```","")
	logger.info(f"LLM Response Cypher for Knowledge Base: {ai_msg}")
	output=""
	try:
		output=graph.query(ai_msg)
		if not output:
			logger.warning("Query didn't generate any output")
			return ""
		response_query=f"""You've just attempted to reach an 
		information database. After querying the database, a total
		of {len(output)} matches were found. Their content is as follows:
		
		"""
		for row in output:
			for k in row.keys():
				response_query+="\n"
				if k in ['name','alias']:
					response_query+=f"value: {row[k]}"
				else:
					response_query+=f"{k}: {row[k]}"
		#response_query+=f"""\n Since this answer has already
		#passed the database filter, you can assume it 
		#answers your question."""
		return response_query
	except Exception as ex:
		logger.error(f"An error occurred: {ex}")
	logger.debug(f"Memgrpah output of LLM query: {output}")
	return output

def chat_loop_graph_questions(config, postgresql_connection, graph, rdf_additional_data):
	"""
	RAG chat using that enables user prompt question and then use KG RAG to answer them

	Params:
		dict (config): Configuration dictionary using values from .yaml file
		psycopg2.connection (postgresql_connection): Database connnection
		langchain_community.graphs.memgraph_graph.MemgraphGraph (graph): Memgraph knowledge graph
		string (rdf_additional_data): Ontology definitions found in file
	"""
	system_prompt=graph_system_prompt(postgresql_connection, config, graph, rdf_additional_data)
	behavior_memgraph=select_prompt(postgresql_connection, config, 7, variables={})
	startChat=True
	if system_prompt is None:
		startChat=False
		print("Error while loading. Please check logs")
	while startChat:
		user_input = input("You: ")
		if user_input.strip().lower() == "/bye":
			print("Goodbye!")
			break
		if not user_input.strip():
			print("Ask a question to the vector dataset or type '/bye' to exit")
		else:
			response = graph_search(config, graph, system_prompt, user_input) 
			if not response:
				response="Could not retrieve information regarding the provided query"
			else:
				query = f"Context: {response} Question: {user_input}"
				logger.debug(f"Query to be processed by last step of KG: {query}")
				messages = [
					{"role": "system", "content": behavior_memgraph},
					{"role": "user", "content": query}
				]
				logger.info("Calling LLM to interpret KG response")
				response = get_chat_completion(config, messages)
			print("Graph: "+response)

def chat_loop(config, postgresql_connection, graph, rdf_additional_data):
	"""
	RAG chat using that enables user prompt question and then use Vector + KG RAG to answer them

	Params:
		dict (config): Configuration dictionary using values from .yaml file
		psycopg2.connection (postgresql_connection): Database connnection
		langchain_community.graphs.memgraph_graph.MemgraphGraph (graph): Memgraph knowledge graph
		string (rdf_additional_data): Ontology definitions found in file
	"""
	system_prompt=graph_system_prompt(postgresql_connection, config,  graph, rdf_additional_data)
	behavior_memgraph=select_prompt(postgresql_connection, config, 7, variables={})
	startChat=True
	if system_prompt is None:
		startChat=False
		print("Error while loading. Please check logs")
	while startChat:
		user_input = input("You: ")
		if user_input.strip().lower() == "/bye":
			print("Goodbye!")
			break
		if not user_input.strip():
			print("Ask a question to the vector dataset or type '/bye' to exit")
		else:
			context_responses={}
			logger.info("Asking Graph...")
			response = graph_search(config, graph, system_prompt, user_input) 
			if not response:
				response="I don't know"
			else:
				query = f"Context: {response} Question: {user_input}"
				logger.debug(f"Query to be processed by last step of KG: {query}")
				messages = [
					{"role": "system", "content": behavior_memgraph},
					{"role": "user", "content": query}
				]
				logger.info("Calling LLM to interpret KG response")
				response = get_chat_completion(config, messages)
			context_responses['KG']=response
			logger.info("Asking Vector...")
			response = prettyfi_vector_search_with_llm(config, postgresql_connection, user_input)
			if response is None or not response:
				response="I don't know"
			context_responses['Vector']=response
			query=""
			for k in context_responses:
				query+=f"{k}: {context_responses[k]}\n"
			query+=f"Question: {user_input}"
			system_prompt_for_both_responses=select_prompt(postgresql_connection, config, 9, variables={'key_1':list(context_responses.keys())[0], 'key_2':list(context_responses.keys())[1]})


			logger.info("Merging answers...")
			messages = [
				{"role": "system", "content": system_prompt_for_both_responses},
				{"role": "user", "content": query}
			]
			response = get_chat_completion(config, messages)
			print("AI: "+response)
